{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af2f5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bb4b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Results experimental analysis 2.2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305cd61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rute', 'System prompt', 'numero_ejemplos', 'model', 'problem',\n",
       "       'iteration', 'System_prompt_modificado', 'Recursividad',\n",
       "       'Errores conceptuales', 'Errores de sintaxis', 'Problem_name',\n",
       "       'iteration_number', 'Category_response'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d58eb48a",
   "metadata": {},
   "source": [
    "This dataset has a row for each evaluation made of the experiment performed to identify if the code has errors. \n",
    "\n",
    "In the column rute we have the rute to the local file with the answer we are analyzing. \n",
    "\n",
    "The column \"System prompt\" is null is the system role has not been modified and if we have modified the system role used. \n",
    "\n",
    "The column \"numero_ejemplos\": can take the value 0, 1 or 2 depending if we are performing zero-shot, 1-shot or 2-zhot prompting. \n",
    "\n",
    "The column \"model\": contains the model used to obtain this answer under study. The options are: \"llama3\", \"llama2\", \"codellama\", \"deepseek\", \"platypus\", \"gpt-3.5\" and \"gpt-4\". \n",
    "\n",
    "The column \"problem\" contains information related to the problem under study, if it's a recursive implementation or not and if it has syntax errors or not. \n",
    "\n",
    "The column \"iteration\" is a number between 0 and 5 and is a way to identify different executions of the same input.\n",
    "\n",
    "The columns \"System_prompt_modificado\", \"Recursividad\", \"Errores conceptuales\" and \"Errores de sintaxis\" can be true or false.\n",
    "\n",
    "In the column Category_response we have the label corresponding to the response we are analysing. This are the possible values it can take:\n",
    "\n",
    "\n",
    "tag_0: \"No response, is not compliant with ethical guidelines\"\n",
    "\n",
    "tag_1: \"Takes information from the example, incorrect answer\"\n",
    "\n",
    "tag_2: \" There are no errors in the code and it does not find errors\"\n",
    "\n",
    "tag_3: \" There are no errors in the code, it gives only code_sintax_error filed, incomplete answer\"\n",
    "\n",
    "tag_4: \"The code has errors, it identifies some of them but not all\"\n",
    "\n",
    "tag_5: \"The code has errors, it does not identify any of them\"\n",
    "\n",
    "tag_6: \"Gives extra information apart from the dictionary asked,does not give an answer aligned with the instructions given\"\n",
    "\n",
    "tag_7: \" The answer provide false errors\"\n",
    "\n",
    "tag_8: \"it gives the full code corrected\"\n",
    "\n",
    "tag_9: \"The code has errors, it identifies all of them\"\n",
    "\n",
    "tag_10: \"It gives corrections that are not correct\"\n",
    "\n",
    "tag_11: \"It gives codes from the training!!!\"\n",
    "\n",
    "tag_12: \"detects the conceptual error in the absence of syntactic errors\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d198ab02",
   "metadata": {},
   "source": [
    "## Codellama exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fab97a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category_response\n",
      "tag_0    355\n",
      "tag_1      5\n",
      "dtype: int64\n",
      "Category_response\n",
      "tag_0    98.611111\n",
      "tag_1     1.388889\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_codellama = df[df.iloc[:,3] =='codellama']\n",
    "\n",
    "pivot_table_codellama = pd.pivot_table(df_codellama, \n",
    "                             index='Category_response', \n",
    "                             aggfunc='size', \n",
    "                             fill_value=0)\n",
    "total_count_codellama = pivot_table_codellama.sum()\n",
    "\n",
    "percentage_table_codellama = (pivot_table_codellama / total_count_codellama) * 100\n",
    "\n",
    "print(pivot_table_codellama)\n",
    "print(percentage_table_codellama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a982e",
   "metadata": {},
   "source": [
    "## Llama2 exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39efcf4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category_response\n",
      "tag_1      35\n",
      "tag_11     10\n",
      "tag_4      20\n",
      "tag_5       7\n",
      "tag_6      29\n",
      "tag_7     243\n",
      "tag_8       7\n",
      "tag_9       9\n",
      "dtype: int64\n",
      "Category_response\n",
      "tag_1      9.722222\n",
      "tag_11     2.777778\n",
      "tag_4      5.555556\n",
      "tag_5      1.944444\n",
      "tag_6      8.055556\n",
      "tag_7     67.500000\n",
      "tag_8      1.944444\n",
      "tag_9      2.500000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_llama2 = df[df.iloc[:,3] =='llama2']\n",
    "\n",
    "pivot_table_llama2 = pd.pivot_table(df_llama2, \n",
    "                             index='Category_response', \n",
    "                             aggfunc='size', \n",
    "                             fill_value=0)\n",
    "total_count_llama2 = pivot_table_llama2.sum()\n",
    "\n",
    "percentage_table_llama2 = (pivot_table_llama2 / total_count_llama2) * 100\n",
    "\n",
    "print(pivot_table_llama2)\n",
    "print(percentage_table_llama2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c6d2c",
   "metadata": {},
   "source": [
    "The llama2 model is similarly irrelevant to the current task. It is of interest to note that 67.5% of the responses include\n",
    "erroneous information, 9.7% of the time the model provides data from the examples,\n",
    "and 8% of the responses are not in the expected format,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca2455",
   "metadata": {},
   "source": [
    "# Results obtained from the llama3, gpt-4, gpt-3.5, deepseek, platypus and qween models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b64a8e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_tags = ['tag_1','tag_6', 'tag_7', 'tag_10', 'tag_8','tag_11'] \n",
    "df['Category_new_1'] = df['Category_response'].apply(lambda x: 'respuesta_invalida' if x in invalid_tags else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f18bc0",
   "metadata": {},
   "source": [
    "We consider as invalid answers those in which errors are given in the examples and not in the code under study, the answer provided is in the wrong format, provides false errors, provides corrections that are not correct, ignores the instructions and gives a whole corrected code or gives training codes in its answer. \n",
    "\n",
    "We would like to see for each model what percentage of their answers are invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ddabefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_percentatge(df, col_filtrar,val_filtrar, col_agrupar):\n",
    "    df_total = df.groupby([col_agrupar]).size().rename('Total')\n",
    "    df_filtrado = df[df.iloc[:, col_filtrar] == val_filtrar]\n",
    "    grouped_df = df_filtrado.groupby([col_agrupar]).size().reset_index(name='Count')\n",
    "    df_new = pd.merge(grouped_df, df_total, on =[col_agrupar])\n",
    "    df_new['percentage'] = (df_new['Count'] / df_new['Total'] * 100).round(1)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9db2b28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       model  Count  Total  percentage\n",
      "0  codellama      5    360         1.4\n",
      "1   deepseek    118    360        32.8\n",
      "2    gpt-3.5     30    216        13.9\n",
      "3      gpt-4      5    216         2.3\n",
      "4     llama2    324    360        90.0\n",
      "5     llama3     41    360        11.4\n",
      "6   platypus     70    360        19.4\n",
      "7       qwen     44    360        12.2\n"
     ]
    }
   ],
   "source": [
    "df_2 = fun_percentatge(df, -1,'respuesta_invalida', 'model')\n",
    "print(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9cdd04",
   "metadata": {},
   "source": [
    "llama2 has been discarded because 90% of its answers are incorrect. codellama has also been discarded because almost all of its answers are that it cannot do what is asked of it for ethical reasons.\n",
    "\n",
    "The models from the one that generated the highest percentage of invalid_answers to the one that generated the lowest percentage of invalid_answers are: 'deepseek', 'platypus', gpt-3.5, qwen, llama3 and gpt-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dea142",
   "metadata": {},
   "source": [
    "We now want to see which models have a tendency not to identify errors. \n",
    "\n",
    "So we have \"tag_5\" which is the code that has errors but does not identify any of them.\n",
    "\n",
    "The initial dataset must be the set of experiments performed on the codes with syntax errors in order to make sense of the percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b08c148f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      model  Count  Total  percentage\n",
      "0   gpt-3.5      2    108         1.9\n",
      "1    llama2      7    180         3.9\n",
      "2  platypus     90    180        50.0\n",
      "3      qwen     35    180        19.4\n"
     ]
    }
   ],
   "source": [
    "df_err =df[df.iloc[:,9] ==True]\n",
    "\n",
    "df_3 = fun_percentatge(df_err, -1,'tag_5', 'model')\n",
    "print(df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982cd49",
   "metadata": {},
   "source": [
    "We see that platyplus does not detect syntax errors 50% of the times it is asked and therefore it is not a good model for this type of task, since half of the times it tells the student that there are no errors, there is a 50% chance that there are. \n",
    "\n",
    "\n",
    "Deepseek worries us because it has given us invalid answers 32.8% of the time. Let's see what kind of invalid answers we are referring to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d656fd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid deepseek responses for code that has no syntax errors \n",
      " \n",
      "  Category_response  Count  Total  percentage\n",
      "0             tag_6      5     15        33.3\n",
      "1             tag_7     10     15        66.7\n",
      "Invalid deepseek responses for code that has syntax errors \n",
      "\n",
      "  Category_response  Count  Total  percentage\n",
      "0            tag_10     37    103        35.9\n",
      "1             tag_7     56    103        54.4\n",
      "2             tag_8     10    103         9.7\n"
     ]
    }
   ],
   "source": [
    "df_deepseek = df[df.iloc[:,3] =='deepseek']\n",
    "df_deepseek_no_val = df_deepseek[df_deepseek.iloc[:,-1] =='respuesta_invalida']\n",
    "df_deepseek_no_val_no_err =df_deepseek_no_val[df_deepseek_no_val.iloc[:,9] ==False]\n",
    "df_4 = fun_percentatge(df_deepseek_no_val_no_err, -1,'respuesta_invalida','Category_response')\n",
    "df_4['Total'] = df_4['Count'].sum()\n",
    "df_4['percentage'] = (df_4['Count'] / df_4['Total'] * 100).round(1)\n",
    "print(\"Invalid deepseek responses for code that has no syntax errors \\n \")\n",
    "print(df_4)\n",
    "\n",
    "df_deepseek_no_val_err =df_deepseek_no_val[df_deepseek_no_val.iloc[:,9] ==True]\n",
    "df_4 = fun_percentatge(df_deepseek_no_val_err, -1,'respuesta_invalida','Category_response')\n",
    "df_4['Total'] = df_4['Count'].sum()\n",
    "df_4['percentage'] = (df_4['Count'] / df_4['Total'] * 100).round(1)\n",
    "print(\"Invalid deepseek responses for code that has syntax errors \\n\")\n",
    "print(df_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920f385",
   "metadata": {},
   "source": [
    "As we have seen that 32.8% (118 out of 360) of deepseek's answers give us invalid answers, we wanted to find out when it gives this kind of answers, if they are when the code is correct or when there are syntax errors and what happens when we modify the system role or when we add examples. \n",
    "\n",
    "There are 180 executions without syntax errors and 180 executions with syntax errors. Only 15 of those without errors have said that they have errors, while 103 of the 180 with errors have not given us a valid answer. \n",
    "\n",
    "Therefore we consider that deepseek is not a valid model for this task either, since when entering codes without errors it responded incorrectly only 8.3% of the time, but when providing codes with errors 57.2% of the time it gave false errors or did not follow the instructions of the prompt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd8c23",
   "metadata": {},
   "source": [
    "We have discarded codellama, llama2, platypus and deepseek for this activity. \n",
    "\n",
    "Let's analyse the gpt-3.5, gpt-4, llama3 and qwen models.\n",
    "\n",
    "1- Codes without syntax errors\n",
    "\n",
    "2- Codes with syntax errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38f50da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rute</th>\n",
       "      <th>System prompt</th>\n",
       "      <th>numero_ejemplos</th>\n",
       "      <th>model</th>\n",
       "      <th>problem</th>\n",
       "      <th>iteration</th>\n",
       "      <th>System_prompt_modificado</th>\n",
       "      <th>Recursividad</th>\n",
       "      <th>Errores conceptuales</th>\n",
       "      <th>Errores de sintaxis</th>\n",
       "      <th>Problem_name</th>\n",
       "      <th>iteration_number</th>\n",
       "      <th>Category_response</th>\n",
       "      <th>Category_new_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Sin_system_prompt/qwen/euclides_rec_sin_errore...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qwen</td>\n",
       "      <td>euclides_rec_sin_errores</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_0</td>\n",
       "      <td>tag_2</td>\n",
       "      <td>tag_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Sin_system_prompt/qwen/euclides_rec_sin_errore...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qwen</td>\n",
       "      <td>euclides_rec_sin_errores</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_1</td>\n",
       "      <td>tag_2</td>\n",
       "      <td>tag_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Sin_system_prompt/qwen/euclides_rec_sin_errore...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qwen</td>\n",
       "      <td>euclides_rec_sin_errores</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_2</td>\n",
       "      <td>tag_2</td>\n",
       "      <td>tag_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Sin_system_prompt/qwen/euclides_rec_sin_errore...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qwen</td>\n",
       "      <td>euclides_rec_sin_errores</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_3</td>\n",
       "      <td>tag_2</td>\n",
       "      <td>tag_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Sin_system_prompt/qwen/euclides_rec_sin_errore...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>qwen</td>\n",
       "      <td>euclides_rec_sin_errores</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_4</td>\n",
       "      <td>tag_2</td>\n",
       "      <td>tag_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>Con_system_prompt/gpt-4/euclides_sin_rec_con_e...</td>\n",
       "      <td>You are an AI expert in detecting if a code ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>euclides_sin_rec_con_errores</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_1</td>\n",
       "      <td>tag_9</td>\n",
       "      <td>tag_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Con_system_prompt/gpt-4/euclides_sin_rec_con_e...</td>\n",
       "      <td>You are an AI expert in detecting if a code ha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>euclides_sin_rec_con_errores</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_2</td>\n",
       "      <td>tag_9</td>\n",
       "      <td>tag_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>Con_system_prompt/gpt-4/euclides_sin_rec_con_e...</td>\n",
       "      <td>You are an AI expert in detecting if a code ha...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>euclides_sin_rec_con_errores</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_0</td>\n",
       "      <td>tag_9</td>\n",
       "      <td>tag_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>Con_system_prompt/gpt-4/euclides_sin_rec_con_e...</td>\n",
       "      <td>You are an AI expert in detecting if a code ha...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>euclides_sin_rec_con_errores</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_1</td>\n",
       "      <td>tag_9</td>\n",
       "      <td>tag_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>Con_system_prompt/gpt-4/euclides_sin_rec_con_e...</td>\n",
       "      <td>You are an AI expert in detecting if a code ha...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>euclides_sin_rec_con_errores</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Euclides</td>\n",
       "      <td>iteration_2</td>\n",
       "      <td>tag_9</td>\n",
       "      <td>tag_9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1152 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   rute  \\\n",
       "180   Sin_system_prompt/qwen/euclides_rec_sin_errore...   \n",
       "181   Sin_system_prompt/qwen/euclides_rec_sin_errore...   \n",
       "182   Sin_system_prompt/qwen/euclides_rec_sin_errore...   \n",
       "183   Sin_system_prompt/qwen/euclides_rec_sin_errore...   \n",
       "184   Sin_system_prompt/qwen/euclides_rec_sin_errore...   \n",
       "...                                                 ...   \n",
       "2588  Con_system_prompt/gpt-4/euclides_sin_rec_con_e...   \n",
       "2589  Con_system_prompt/gpt-4/euclides_sin_rec_con_e...   \n",
       "2590  Con_system_prompt/gpt-4/euclides_sin_rec_con_e...   \n",
       "2591  Con_system_prompt/gpt-4/euclides_sin_rec_con_e...   \n",
       "2592  Con_system_prompt/gpt-4/euclides_sin_rec_con_e...   \n",
       "\n",
       "                                          System prompt  numero_ejemplos  \\\n",
       "180                                                 NaN              0.0   \n",
       "181                                                 NaN              0.0   \n",
       "182                                                 NaN              0.0   \n",
       "183                                                 NaN              0.0   \n",
       "184                                                 NaN              0.0   \n",
       "...                                                 ...              ...   \n",
       "2588  You are an AI expert in detecting if a code ha...              1.0   \n",
       "2589  You are an AI expert in detecting if a code ha...              1.0   \n",
       "2590  You are an AI expert in detecting if a code ha...              2.0   \n",
       "2591  You are an AI expert in detecting if a code ha...              2.0   \n",
       "2592  You are an AI expert in detecting if a code ha...              2.0   \n",
       "\n",
       "      model                       problem  iteration  \\\n",
       "180    qwen      euclides_rec_sin_errores        0.0   \n",
       "181    qwen      euclides_rec_sin_errores        1.0   \n",
       "182    qwen      euclides_rec_sin_errores        2.0   \n",
       "183    qwen      euclides_rec_sin_errores        3.0   \n",
       "184    qwen      euclides_rec_sin_errores        4.0   \n",
       "...     ...                           ...        ...   \n",
       "2588  gpt-4  euclides_sin_rec_con_errores        1.0   \n",
       "2589  gpt-4  euclides_sin_rec_con_errores        2.0   \n",
       "2590  gpt-4  euclides_sin_rec_con_errores        0.0   \n",
       "2591  gpt-4  euclides_sin_rec_con_errores        1.0   \n",
       "2592  gpt-4  euclides_sin_rec_con_errores        2.0   \n",
       "\n",
       "      System_prompt_modificado  Recursividad  Errores conceptuales  \\\n",
       "180                      False          True                 False   \n",
       "181                      False          True                 False   \n",
       "182                      False          True                 False   \n",
       "183                      False          True                 False   \n",
       "184                      False          True                 False   \n",
       "...                        ...           ...                   ...   \n",
       "2588                      True         False                 False   \n",
       "2589                      True         False                 False   \n",
       "2590                      True         False                 False   \n",
       "2591                      True         False                 False   \n",
       "2592                      True         False                 False   \n",
       "\n",
       "      Errores de sintaxis Problem_name iteration_number Category_response  \\\n",
       "180                   0.0     Euclides      iteration_0             tag_2   \n",
       "181                   0.0     Euclides      iteration_1             tag_2   \n",
       "182                   0.0     Euclides      iteration_2             tag_2   \n",
       "183                   0.0     Euclides      iteration_3             tag_2   \n",
       "184                   0.0     Euclides      iteration_4             tag_2   \n",
       "...                   ...          ...              ...               ...   \n",
       "2588                  1.0     Euclides      iteration_1             tag_9   \n",
       "2589                  1.0     Euclides      iteration_2             tag_9   \n",
       "2590                  1.0     Euclides      iteration_0             tag_9   \n",
       "2591                  1.0     Euclides      iteration_1             tag_9   \n",
       "2592                  1.0     Euclides      iteration_2             tag_9   \n",
       "\n",
       "     Category_new_1  \n",
       "180           tag_2  \n",
       "181           tag_2  \n",
       "182           tag_2  \n",
       "183           tag_2  \n",
       "184           tag_2  \n",
       "...             ...  \n",
       "2588          tag_9  \n",
       "2589          tag_9  \n",
       "2590          tag_9  \n",
       "2591          tag_9  \n",
       "2592          tag_9  \n",
       "\n",
       "[1152 rows x 14 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = ['gpt-3.5', 'gpt-4', 'llama3', 'qwen']\n",
    "sub_df = df[df['model'].isin(mod)]\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "296f398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#codes without syntax errors\n",
    "sub_df_no_err =sub_df[sub_df.iloc[:,9] ==False]\n",
    "\n",
    "#codes with syntax errors\n",
    "sub_df_err =sub_df[sub_df.iloc[:,9] ==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2752749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     model  Count  Total  percentage\n",
      "0  gpt-3.5     19    108        17.6\n",
      "1    gpt-4    102    108        94.4\n",
      "2   llama3    180    180       100.0\n",
      "3     qwen    160    180        88.9\n"
     ]
    }
   ],
   "source": [
    "#of the codes that do not have errors we must group them by model and see what percentage of the answers are \n",
    "#tag_2 (no errors and no errors found) and what percentage is tag_3 provides a dictionary with only the first field\n",
    "\n",
    "df_5 = fun_percentatge(sub_df_no_err, -1,'tag_2', 'model')\n",
    "print(df_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0df4011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     model  Count  Total  percentage\n",
      "0  gpt-3.5     86    108        79.6\n",
      "1     qwen     20    180        11.1\n"
     ]
    }
   ],
   "source": [
    "df_6 = fun_percentatge(sub_df_no_err, -1,'tag_3', 'model')\n",
    "print(df_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "589bb0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     model  Count  Total  percentage\n",
      "0  gpt-3.5      3    108         2.8\n"
     ]
    }
   ],
   "source": [
    "df_7 = fun_percentatge(sub_df_no_err, -1,'respuesta_invalida', 'model')\n",
    "print(df_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66d60a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Category_new_1</th>\n",
       "      <th>respuesta_invalida</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>Category_response</th>\n",
       "      <th>Problem_name</th>\n",
       "      <th>System_prompt_modificado</th>\n",
       "      <th>numero_ejemplos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">gpt-3.5</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">tag_7</th>\n",
       "      <th>Eratostenes</th>\n",
       "      <th>True</th>\n",
       "      <th>0.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Knapsack</th>\n",
       "      <th>True</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category_new_1                                                                   respuesta_invalida\n",
       "model   Category_response Problem_name System_prompt_modificado numero_ejemplos                    \n",
       "gpt-3.5 tag_7             Eratostenes  True                     0.0                               1\n",
       "                          Knapsack     True                     0.0                               2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df_no_err[sub_df_no_err['Category_new_1']=='respuesta_invalida'].groupby(['model','Category_response', 'Problem_name', 'System_prompt_modificado', 'numero_ejemplos' ])['Category_new_1'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108daa31",
   "metadata": {},
   "source": [
    "When the entered codes have no syntax errors they practically always say that there are no errors. However, 11.1% of the time for qwen and 79.6% of the time for gpt-3.5 we get a dictionary with only the first field. This is because for them the instructions are not clear and we see the importance of a good prompting since telling them what to do if there are errors but not telling them what to do when there are no errors confuses them. \n",
    "\n",
    "Finally we see that there are 3 responses out of 108 runs made to gpt-3.5 with errors where we get false errors. These are minor errors such as typing errors or missing commas that can come from the system prompt telling it to look for errors.\n",
    "\n",
    "These incorrect answers are:\n",
    "\n",
    "The one from eratostenes:\n",
    "\n",
    "{\n",
    "    \"code_sintax_error\": \"Yes\",\n",
    "    \"code_sintax_explanation\": \"There is a syntax error in the code. The issue is with the indentation in the function definition. The 'def fun_1(x, p=None, c=2):' line should not have extra indentation, it should be aligned with the left margin.\"\n",
    "}\n",
    "\n",
    "Knapsack's:\n",
    "\n",
    "{\n",
    "    \"code_sintax_error\": \"Yes\",\n",
    "    \"code_sintax_explanation\": \"There are syntax errors in the provided code. The function `fun_1` seems to have indentation issues, as the `def`, `if`, and `else` statements are not properly aligned. Additionally, there is a space missing in the expression `n- 1` within the function. These issues need to be corrected to ensure proper syntax in Python.\"\n",
    "}\n",
    "\n",
    "{\n",
    "    \"code_sintax_error\": \"Yes\",\n",
    "    \"code_sintax_explanation\": \"There is a syntax error in the code. The comma after 'return 0' is incorrect. Python expects either a single value or a tuple, not both.\"\n",
    "}\n",
    "\n",
    "These are errors that are not real and yet the student can understand that sometimes the model is wrong and is not telling them to change big things in the code. As we can see, sometimes the models are wrong.\n",
    "\n",
    "Let's look now at the ones that have errors in the code entered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48f8623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are errors identify some of them but not all:\n",
      "\n",
      "     model  Count  Total  percentage\n",
      "0  gpt-3.5     39    108        36.1\n",
      "1    gpt-4      7    108         6.5\n",
      "2   llama3     14    180         7.8\n",
      "3     qwen     80    180        44.4\n",
      "There are mistakes it identifies all of them:\n",
      "\n",
      "     model  Count  Total  percentage\n",
      "0  gpt-3.5     40    108        37.0\n",
      "1    gpt-4     96    108        88.9\n",
      "2   llama3    125    180        69.4\n",
      "3     qwen     21    180        11.7\n",
      "There are errors and it gives invalid answers:\n",
      "\n",
      "     model  Count  Total  percentage\n",
      "0  gpt-3.5     27    108        25.0\n",
      "1    gpt-4      5    108         4.6\n",
      "2   llama3     41    180        22.8\n",
      "3     qwen     44    180        24.4\n",
      "There are errors but it does not identify them:\n",
      "\n",
      "     model  Count  Total  percentage\n",
      "0  gpt-3.5      2    108         1.9\n",
      "1     qwen     35    180        19.4\n"
     ]
    }
   ],
   "source": [
    "#there are errors identify some of them but not all.\n",
    "print(\"There are errors identify some of them but not all:\\n\")\n",
    "df_8 = fun_percentatge(sub_df_err, -1,'tag_4', 'model')\n",
    "print(df_8)\n",
    "\n",
    "\n",
    "#there are mistakes it identifies all of them\n",
    "print(\"There are mistakes it identifies all of them:\\n\")\n",
    "df_9 = fun_percentatge(sub_df_err, -1,'tag_9', 'model')\n",
    "print(df_9)\n",
    "\n",
    "#gives invalid answers\n",
    "print(\"There are errors and it gives invalid answers:\\n\")\n",
    "df_10 = fun_percentatge(sub_df_err, -1,'respuesta_invalida', 'model')\n",
    "print(df_10)\n",
    "\n",
    "#fails to identify errors\n",
    "print(\"There are errors but it does not identify them:\\n\")\n",
    "df_11 = fun_percentatge(sub_df_err, -1,'tag_5', 'model')\n",
    "print(df_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db1292",
   "metadata": {},
   "source": [
    "Now we will go model by model. The best is gpt-4 as 88% of the time it identifies all the errors, followed by llama3 with 69.4%, gpt with 37% and finally qwen which only 11.7% of the time gives all the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "055ceaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>gpt-4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <th>Category_response</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">busqueda_binaria_sin_rec_con_errores</th>\n",
       "      <th>tag_10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">eratostenes_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eratostenes_sin_rec_con_errores</th>\n",
       "      <th>tag_9</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">euclides_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euclides_sin_rec_con_errores</th>\n",
       "      <th>tag_9</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">knapsack_rec_con_errores</th>\n",
       "      <th>tag_10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                                                   gpt-4\n",
       "problem                              Category_response       \n",
       "busqueda_binaria_sin_rec_con_errores tag_10                 1\n",
       "                                     tag_9                 17\n",
       "eratostenes_rec_con_errores          tag_4                  4\n",
       "                                     tag_9                 14\n",
       "eratostenes_sin_rec_con_errores      tag_9                 18\n",
       "euclides_rec_con_errores             tag_4                  3\n",
       "                                     tag_9                 15\n",
       "euclides_sin_rec_con_errores         tag_9                 18\n",
       "knapsack_rec_con_errores             tag_10                 1\n",
       "                                     tag_7                  3\n",
       "                                     tag_9                 14"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpt-4\n",
    "\n",
    "sub_df_err[sub_df_err['model']=='gpt-4'].groupby(['problem','Category_response'])['model'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fb23a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>gpt-3.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <th>Category_response</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">busqueda_binaria_sin_rec_con_errores</th>\n",
       "      <th>tag_10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">eratostenes_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">eratostenes_sin_rec_con_errores</th>\n",
       "      <th>tag_1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_10</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">euclides_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_5</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">euclides_sin_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">knapsack_rec_con_errores</th>\n",
       "      <th>tag_7</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                                                   gpt-3.5\n",
       "problem                              Category_response         \n",
       "busqueda_binaria_sin_rec_con_errores tag_10                   1\n",
       "                                     tag_4                    7\n",
       "                                     tag_7                    3\n",
       "                                     tag_9                    7\n",
       "eratostenes_rec_con_errores          tag_4                    6\n",
       "                                     tag_7                    2\n",
       "                                     tag_9                   10\n",
       "eratostenes_sin_rec_con_errores      tag_1                    1\n",
       "                                     tag_10                   4\n",
       "                                     tag_7                    2\n",
       "                                     tag_9                   11\n",
       "euclides_rec_con_errores             tag_4                   11\n",
       "                                     tag_5                    2\n",
       "                                     tag_7                    4\n",
       "                                     tag_9                    1\n",
       "euclides_sin_rec_con_errores         tag_4                   15\n",
       "                                     tag_9                    3\n",
       "knapsack_rec_con_errores             tag_7                   10\n",
       "                                     tag_9                    8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gpt-3.5\n",
    "\n",
    "sub_df_err[sub_df_err['model']=='gpt-3.5'].groupby(['problem','Category_response'])['model'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc275c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>llama3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <th>Category_response</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">busqueda_binaria_sin_rec_con_errores</th>\n",
       "      <th>tag_7</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">eratostenes_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eratostenes_sin_rec_con_errores</th>\n",
       "      <th>tag_9</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">euclides_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">euclides_sin_rec_con_errores</th>\n",
       "      <th>tag_10</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">knapsack_rec_con_errores</th>\n",
       "      <th>tag_10</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                                                   llama3\n",
       "problem                              Category_response        \n",
       "busqueda_binaria_sin_rec_con_errores tag_7                  10\n",
       "                                     tag_9                  20\n",
       "eratostenes_rec_con_errores          tag_4                   4\n",
       "                                     tag_9                  26\n",
       "eratostenes_sin_rec_con_errores      tag_9                  30\n",
       "euclides_rec_con_errores             tag_4                   5\n",
       "                                     tag_7                  11\n",
       "                                     tag_9                  14\n",
       "euclides_sin_rec_con_errores         tag_10                  5\n",
       "                                     tag_7                   5\n",
       "                                     tag_9                  20\n",
       "knapsack_rec_con_errores             tag_10                  7\n",
       "                                     tag_4                   5\n",
       "                                     tag_7                   3\n",
       "                                     tag_9                  15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#llama3\n",
    "\n",
    "sub_df_err[sub_df_err['model']=='llama3'].groupby(['problem','Category_response'])['model'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c8c625b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>qwen</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>problem</th>\n",
       "      <th>Category_response</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">busqueda_binaria_sin_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">eratostenes_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eratostenes_sin_rec_con_errores</th>\n",
       "      <th>tag_5</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">euclides_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_7</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">euclides_sin_rec_con_errores</th>\n",
       "      <th>tag_4</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_5</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">knapsack_rec_con_errores</th>\n",
       "      <th>tag_10</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tag_9</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                                                   qwen\n",
       "problem                              Category_response      \n",
       "busqueda_binaria_sin_rec_con_errores tag_4                10\n",
       "                                     tag_7                14\n",
       "                                     tag_9                 6\n",
       "eratostenes_rec_con_errores          tag_4                25\n",
       "                                     tag_7                 5\n",
       "eratostenes_sin_rec_con_errores      tag_5                30\n",
       "euclides_rec_con_errores             tag_4                20\n",
       "                                     tag_7                 5\n",
       "                                     tag_9                 5\n",
       "euclides_sin_rec_con_errores         tag_4                25\n",
       "                                     tag_5                 5\n",
       "knapsack_rec_con_errores             tag_10               20\n",
       "                                     tag_9                10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qwen\n",
    "\n",
    "sub_df_err[sub_df_err['model']=='qwen'].groupby(['problem','Category_response'])['model'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2dd79",
   "metadata": {},
   "source": [
    "gpt-4 is the only one that practically never gives invalid answers. Only 6 times out of 108, for the other models around 25% give invalid answers. Let us analyse what these invalid answers were.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eeb77aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category_new_1</th>\n",
       "      <th>respuesta_invalida</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>numero_ejemplos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">gpt-3.5</th>\n",
       "      <th>0.0</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">gpt-4</th>\n",
       "      <th>0.0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">llama3</th>\n",
       "      <th>0.0</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">qwen</th>\n",
       "      <th>0.0</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category_new_1           respuesta_invalida\n",
       "model   numero_ejemplos                    \n",
       "gpt-3.5 0.0                              13\n",
       "        1.0                               4\n",
       "        2.0                              10\n",
       "gpt-4   0.0                               2\n",
       "        1.0                               1\n",
       "        2.0                               2\n",
       "llama3  0.0                              25\n",
       "        1.0                              16\n",
       "qwen    0.0                              20\n",
       "        1.0                              15\n",
       "        2.0                               9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df_err[sub_df_err['Category_new_1']=='respuesta_invalida'].groupby(['model', 'numero_ejemplos' ])['Category_new_1'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105bcb07",
   "metadata": {},
   "source": [
    "we see that by introducing examples the number of wrong answers reduces. In the case of llama3 there are no invalid responses when two examples are given, for qwen the errors are reduced as more examples are given and for the gpt models it seems that two errors are worse than with only one example, so maybe the input is too long.\n",
    "\n",
    "now let's see what happens when modifying or not modifying the system role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "269d5eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category_new_1</th>\n",
       "      <th>respuesta_invalida</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>System_prompt_modificado</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">False</th>\n",
       "      <th>gpt-3.5</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">True</th>\n",
       "      <th>gpt-3.5</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama3</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category_new_1                    respuesta_invalida\n",
       "System_prompt_modificado model                      \n",
       "False                    gpt-3.5                  15\n",
       "                         gpt-4                     3\n",
       "                         llama3                   15\n",
       "                         qwen                     19\n",
       "True                     gpt-3.5                  12\n",
       "                         gpt-4                     2\n",
       "                         llama3                   26\n",
       "                         qwen                     25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df_err[sub_df_err['Category_new_1']=='respuesta_invalida'].groupby(['System_prompt_modificado','model'])['Category_new_1'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b814972b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Category_new_1</th>\n",
       "      <th>respuesta_invalida</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>numero_ejemplos</th>\n",
       "      <th>System_prompt_modificado</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">gpt-3.5</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th>False</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>False</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2.0</th>\n",
       "      <th>False</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">gpt-4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th>False</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>False</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2.0</th>\n",
       "      <th>False</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">llama3</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th>False</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>False</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">qwen</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th>False</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>False</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <th>False</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category_new_1                                    respuesta_invalida\n",
       "model   numero_ejemplos System_prompt_modificado                    \n",
       "gpt-3.5 0.0             False                                      6\n",
       "                        True                                       7\n",
       "        1.0             False                                      3\n",
       "                        True                                       1\n",
       "        2.0             False                                      6\n",
       "                        True                                       4\n",
       "gpt-4   0.0             False                                      1\n",
       "                        True                                       1\n",
       "        1.0             False                                      1\n",
       "        2.0             False                                      1\n",
       "                        True                                       1\n",
       "llama3  0.0             False                                     10\n",
       "                        True                                      15\n",
       "        1.0             False                                      5\n",
       "                        True                                      11\n",
       "qwen    0.0             False                                      5\n",
       "                        True                                      15\n",
       "        1.0             False                                      5\n",
       "                        True                                      10\n",
       "        2.0             False                                      9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df_err[sub_df_err['Category_new_1']=='respuesta_invalida'].groupby(['model','numero_ejemplos','System_prompt_modificado' ])['Category_new_1'].value_counts().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56b2d8",
   "metadata": {},
   "source": [
    "We see that modifying the system prompt if we don't add examples increases the number of invalid responses for both gpt-3.5, llama3 and qwen. However, if we add two examples and modify the system role, there are no invalid responses for either qwen or llama3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
